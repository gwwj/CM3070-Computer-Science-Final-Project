{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c3ce7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import Swinv2Model\n",
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
    "from torchvision.transforms import functional as F\n",
    "import os\n",
    "import collections\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pydicom\n",
    "\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b40ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = pd.read_csv(\"./Original CSVs/metadata.csv\")\n",
    "df_calc_train = pd.read_csv(\"./Original CSVs/calc_case_description_train_set.csv\")\n",
    "df_calc_test = pd.read_csv(\"./Original CSVs/calc_case_description_test_set.csv\")\n",
    "df_mass_train = pd.read_csv(\"./Original CSVs/mass_case_description_train_set.csv\")\n",
    "df_mass_test = pd.read_csv(\"./Original CSVs/mass_case_description_test_set.csv\")\n",
    "\n",
    "df_fullimages_metadata = pd.DataFrame()\n",
    "df_fullimages_metadata = df_metadata.loc[df_metadata[\"Series Description\"] == \"full mammogram images\"]\n",
    "\n",
    "df_croppedimages_metadata = pd.DataFrame()\n",
    "df_croppedimages_metadata = df_metadata.loc[df_metadata[\"Series Description\"] == \"cropped images\"]\n",
    "\n",
    "df_roiimages_metadata = pd.DataFrame()\n",
    "df_roiimages_metadata = df_metadata.loc[df_metadata[\"Series Description\"] == \"ROI mask images\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d9324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dicom_height(dicom_path):\n",
    "    dicom_file = pydicom.dcmread(dicom_path)\n",
    "    return dicom_file.Rows\n",
    "\n",
    "def get_dicom_width(dicom_path):\n",
    "    dicom_file = pydicom.dcmread(dicom_path)\n",
    "    return dicom_file.Columns\n",
    "\n",
    "def process_dataframe(df_input, df_fullimages_metadata, df_roiimages_metadata):\n",
    "\n",
    "    # Copying the relevant columns from the original dataframe\n",
    "    df_output = df_input[[\"patient_id\", \"abnormality type\", \"pathology\"]].copy()\n",
    "    df_output[\"pathology\"] = df_output[\"pathology\"].replace(\"BENIGN_WITHOUT_CALLBACK\", \"BENIGN\")\n",
    "\n",
    "    # Splitting out the first part of the path to get the ID\n",
    "    df_output[\"full image id\"] = df_input[\"image file path\"].apply(lambda x: x.split('/')[0])\n",
    "    df_output[\"roi mask id\"] = df_input[\"ROI mask file path\"].apply(lambda x: x.split('/')[0])\n",
    "\n",
    "    # Check to make sure that the ID is present in the metadata file, if not they are dropped as the image does not exist in the dataset\n",
    "    df_output[\"full image id isin\"] = df_output[\"full image id\"].isin(df_fullimages_metadata[\"Subject ID\"])\n",
    "    df_output[\"roi mask id isin\"] = df_output[\"roi mask id\"].isin(df_roiimages_metadata[\"Subject ID\"])\n",
    "    df_output = df_output.drop(df_output[df_output[\"full image id isin\"] == False].index)\n",
    "    df_output = df_output.drop(df_output[df_output[\"roi mask id isin\"] == False].index)\n",
    "\n",
    "    # Getting the actual full image path\n",
    "    df_output[\"full image path\"] = df_output.apply(\n",
    "        lambda row: df_fullimages_metadata.loc[df_fullimages_metadata[\"Subject ID\"] == row[\"full image id\"], \"File Location\"].item(),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Checking to make sure the full image file exists if not the row is dropped\n",
    "    df_output[\"full image path\"] = df_output[\"full image path\"].apply(lambda x: x.replace(\"\\\\\", \"/\")+\"/1-1.dcm\")\n",
    "    df_output[\"full image path exists\"] = df_output[\"full image path\"].apply(lambda x: os.path.exists(x))\n",
    "    df_output = df_output.drop(df_output[df_output[\"full image path exists\"] == False].index)\n",
    "\n",
    "    # # Recording the full image dimensions\n",
    "    # df_output[\"full image height\"] = df_output[\"full image path\"].apply(get_dicom_height)\n",
    "    # df_output[\"full image width\"] = df_output[\"full image path\"].apply(get_dicom_width)\n",
    "\n",
    "    # Creating the full image png path\n",
    "    df_output[\"full image png\"] = df_output[\"full image id\"].apply(lambda x: \"./PNGs/\" + x + \".png\")\n",
    "\n",
    "    # Getting the actual roi image path\n",
    "    df_output[\"roi image path\"] = df_output.apply(\n",
    "        lambda row: df_roiimages_metadata.loc[df_roiimages_metadata[\"Subject ID\"] == row[\"roi mask id\"], \"File Location\"].item(),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Checking to make sure the roi image file exists if not the row is dropped\n",
    "    df_output[\"roi image path\"] = df_output[\"roi image path\"].apply(lambda x: x.replace(\"\\\\\", \"/\")+\"/1-2.dcm\")\n",
    "    df_output[\"roi image path exists\"] = df_output[\"roi image path\"].apply(lambda x: os.path.exists(x))\n",
    "    df_output = df_output.drop(df_output[df_output[\"roi image path exists\"] == False].index)\n",
    "\n",
    "    # # Recording the roi image dimensions\n",
    "    # df_output[\"roi image height\"] = df_output[\"roi image path\"].apply(get_dicom_height)\n",
    "    # df_output[\"roi image width\"] = df_output[\"roi image path\"].apply(get_dicom_width)\n",
    "\n",
    "    # Creating the full image png path\n",
    "    df_output[\"roi image png\"] = df_output[\"roi mask id\"].apply(lambda x: \"./PNGs/\" + x + \".png\")\n",
    "\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2940524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all dataframes and save into processed csv files to save on compute time if a rerun is required\n",
    "\n",
    "# df_calc_train_processed = process_dataframe(df_calc_train, df_fullimages_metadata, df_roiimages_metadata)\n",
    "# df_calc_test_processed = process_dataframe(df_calc_test, df_fullimages_metadata, df_roiimages_metadata)\n",
    "# df_mass_train_processed = process_dataframe(df_mass_train, df_fullimages_metadata, df_roiimages_metadata)\n",
    "# df_mass_test_processed = process_dataframe(df_mass_test, df_fullimages_metadata, df_roiimages_metadata)\n",
    "\n",
    "# df_calc_train_processed.to_csv(\"./Processed CSVs/df_calc_train_processed.csv\")\n",
    "# df_calc_test_processed.to_csv(\"./Processed CSVs/df_calc_test_processed.csv\")\n",
    "# df_mass_train_processed.to_csv(\"./Processed CSVs/df_mass_train_processed.csv\")\n",
    "# df_mass_test_processed.to_csv(\"./Processed CSVs/df_mass_test_processed.csv\")\n",
    "\n",
    "df_calc_train_processed = pd.read_csv(\"./Processed CSVs/df_calc_train_processed.csv\")\n",
    "df_calc_test_processed = pd.read_csv(\"./Processed CSVs/df_calc_test_processed.csv\")\n",
    "df_mass_train_processed = pd.read_csv(\"./Processed CSVs/df_mass_train_processed.csv\")\n",
    "df_mass_test_processed = pd.read_csv(\"./Processed CSVs/df_mass_test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906fdd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert the dicom images to png while maintaining aspect ratio\n",
    "\n",
    "def dicom_to_padded_png(dicom_path, output_path, dtype, size=(1024, 1024)):\n",
    "    ds = pydicom.dcmread(dicom_path)\n",
    "    pixel_array = ds.pixel_array\n",
    "\n",
    "    # Normalize based on min/max values\n",
    "    pixel_array = (pixel_array - np.min(pixel_array)) / (np.max(pixel_array) - np.min(pixel_array))\n",
    "    pixel_array = (pixel_array * 255).astype(dtype)\n",
    "\n",
    "    # Create a PIL Image from the pixel array\n",
    "    img = Image.fromarray(pixel_array)\n",
    "\n",
    "    # Get the new size while maintaining aspect ratio\n",
    "    original_width, original_height = img.size\n",
    "    target_width, target_height = size\n",
    "\n",
    "    # Scale the image\n",
    "    scale = min(target_width / original_width, target_height / original_height)\n",
    "    new_width = int(original_width * scale)\n",
    "    new_height = int(original_height * scale)\n",
    "\n",
    "    # Resize the image using the Lanczos filter for maximum image quality\n",
    "    resized_img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "\n",
    "    # Create a new blank square image with black padding\n",
    "    padded_img = Image.new(\"L\", size)\n",
    "    left = (target_width - new_width) // 2\n",
    "    top = (target_height - new_height) // 2\n",
    "    padded_img.paste(resized_img, (left, top))\n",
    "\n",
    "    # Save the padded image as PNG\n",
    "    padded_img.save(output_path)\n",
    "\n",
    "def dicom_to_png(dicom_path, png_path, dtype):\n",
    "    dicom_file = pydicom.dcmread(dicom_path)\n",
    "    pixel_array = dicom_file.pixel_array\n",
    "    pixel_array = pixel_array.astype(dtype)\n",
    "    img = Image.fromarray(pixel_array)\n",
    "    img.save(png_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830f12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert calc train images to png\n",
    "\n",
    "for index, row in df_calc_train_processed.iterrows():\n",
    "    dicom_to_padded_png(row[\"full image path\"], row[\"full image png\"], np.uint16)\n",
    "    dicom_to_padded_png(row[\"roi image path\"], row[\"roi image png\"], np.uint8)\n",
    "\n",
    "print(\"Complete df_calc_train_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a8807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert calc test images to png\n",
    "\n",
    "for index, row in df_calc_test_processed.iterrows():\n",
    "    dicom_to_padded_png(row[\"full image path\"], row[\"full image png\"], np.uint16)\n",
    "    dicom_to_padded_png(row[\"roi image path\"], row[\"roi image png\"], np.uint8)\n",
    "\n",
    "print(\"Complete df_calc_test_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f9e432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert mass train images to png\n",
    "\n",
    "for index, row in df_mass_train_processed.iterrows():\n",
    "    dicom_to_padded_png(row[\"full image path\"], row[\"full image png\"], np.uint16)\n",
    "    dicom_to_padded_png(row[\"roi image path\"], row[\"roi image png\"], np.uint8)\n",
    "\n",
    "print(\"Complete df_mass_train_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9331579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert mass test images to png\n",
    "\n",
    "for index, row in df_mass_test_processed.iterrows():\n",
    "    dicom_to_padded_png(row[\"full image path\"], row[\"full image png\"], np.uint16)\n",
    "    dicom_to_padded_png(row[\"roi image path\"], row[\"roi image png\"], np.uint8)\n",
    "\n",
    "print(\"Complete df_mass_test_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf7551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_boxes(mask_path):\n",
    " \n",
    "    # Open the image and convert it to grayscale\n",
    "    with Image.open(mask_path).convert(\"L\") as img:\n",
    "        # Get the bounding box of all non-zero regions\n",
    "        bbox = img.getbbox()\n",
    "        if bbox:\n",
    "            # Pillow's getbbox returns (left, upper, right, lower)\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            return (x_min, y_min, x_max, y_max)\n",
    "        else:\n",
    "            # The image is completely black\n",
    "            print(\"There is an empty image\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad0d7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_calc_train_processed[\"roi bb coordinates\"] = df_calc_train_processed[\"roi image png\"].apply(get_bounding_boxes)\n",
    "df_calc_test_processed[\"roi bb coordinates\"] = df_calc_test_processed[\"roi image png\"].apply(get_bounding_boxes)\n",
    "df_mass_train_processed[\"roi bb coordinates\"] = df_mass_train_processed[\"roi image png\"].apply(get_bounding_boxes)\n",
    "df_mass_test_processed[\"roi bb coordinates\"] = df_mass_test_processed[\"roi image png\"].apply(get_bounding_boxes)\n",
    "\n",
    "# df_calc_train_processed.to_csv(\"./Processed CSVs/df_calc_train_processed.csv\")\n",
    "# df_calc_test_processed.to_csv(\"./Processed CSVs/df_calc_test_processed.csv\")\n",
    "# df_mass_train_processed.to_csv(\"./Processed CSVs/df_mass_train_processed.csv\")\n",
    "# df_mass_test_processed.to_csv(\"./Processed CSVs/df_mass_test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef580df",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_training = pd.concat([df_calc_train_processed, df_mass_train_processed], ignore_index=True)\n",
    "all_test = pd.concat([df_calc_test_processed, df_mass_test_processed], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4d4ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBISDDSMDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        # Define the class mapping for classification\n",
    "        self.class_map = {\n",
    "            'BENIGN': 0,\n",
    "            'MALIGNANT': 1,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the row for the current index\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_path = row['roi image png']\n",
    "        \n",
    "        # Load the image and convert to RGB\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # The bounding box coordinates are already a tuple.\n",
    "        boxes = [list(row['roi bb coordinates'])]\n",
    "        \n",
    "        # Calculate a square crop around the bounding box\n",
    "        x_min, y_min, x_max, y_max = boxes[0]\n",
    "        center_x = (x_min + x_max) / 2\n",
    "        center_y = (y_min + y_max) / 2\n",
    "        \n",
    "        # Ensure the crop is a square that fully contains the bounding box\n",
    "        box_width = x_max - x_min\n",
    "        box_height = y_max - y_min\n",
    "        max_dim = max(box_width, box_height) * 1.2\n",
    "\n",
    "        # Define crop coordinates\n",
    "        crop_x_min = int(center_x - max_dim / 2)\n",
    "        crop_y_min = int(center_y - max_dim / 2)\n",
    "        crop_x_max = int(center_x + max_dim / 2)\n",
    "        crop_y_max = int(center_y + max_dim / 2)\n",
    "\n",
    "        # Clamp crop coordinates to image boundaries\n",
    "        original_width, original_height = img.size\n",
    "        crop_x_min = max(0, crop_x_min)\n",
    "        crop_y_min = max(0, crop_y_min)\n",
    "        crop_x_max = min(original_width, crop_x_max)\n",
    "        crop_y_max = min(original_height, crop_y_max)\n",
    "\n",
    "        # Perform the crop\n",
    "        cropped_img = F.crop(img, crop_y_min, crop_x_min, crop_y_max - crop_y_min, crop_x_max - crop_x_min)\n",
    "\n",
    "        # Resize the cropped image to 384x384\n",
    "        resized_img = F.resize(cropped_img, (384, 384))\n",
    "\n",
    "        # Calculate scaling factors for bounding box coordinates\n",
    "        cropped_width, cropped_height = cropped_img.size\n",
    "        new_width, new_height = resized_img.size\n",
    "        width_scale = new_width / cropped_width\n",
    "        height_scale = new_height / cropped_height\n",
    "\n",
    "        # Scale bounding box coordinates to match the new image size and crop\n",
    "        scaled_boxes = []\n",
    "        for box in boxes:\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            scaled_x_min = (x_min - crop_x_min) * width_scale\n",
    "            scaled_y_min = (y_min - crop_y_min) * height_scale\n",
    "            scaled_x_max = (x_max - crop_x_min) * width_scale\n",
    "            scaled_y_max = (y_max - crop_y_min) * height_scale\n",
    "            scaled_boxes.append([scaled_x_min, scaled_y_min, scaled_x_max, scaled_y_max])\n",
    "\n",
    "        boxes = torch.as_tensor(scaled_boxes, dtype=torch.float32)\n",
    "        labels = torch.ones((len(boxes),), dtype=torch.int64)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        img_tensor = F.to_tensor(resized_img)\n",
    "\n",
    "        # Get the classification label for the Swin UNETR\n",
    "        class_label = self.class_map[row['pathology']]\n",
    "        class_label_tensor = torch.as_tensor([class_label], dtype=torch.long)\n",
    "\n",
    "        return img_tensor, target, class_label_tensor\n",
    "\n",
    "# Collate function for the DataLoader\n",
    "def collate_fn(batch):\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    class_labels = [item[2] for item in batch]\n",
    "    return images, targets, class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d590e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faster_rcnn_model(num_classes):\n",
    "    backbone_model = Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to24-192to384-22kto1k-ft\", add_pooling_layer=False)\n",
    "    in_channels_list = [192, 384, 768, 1536]\n",
    "    out_channels = 256\n",
    "\n",
    "    class SwinFPNBackbone(nn.Module):\n",
    "        def __init__(self, swin_model, in_channels_list, out_channels):\n",
    "            super().__init__()\n",
    "            self.swin_model = swin_model\n",
    "            self.fpn = FeaturePyramidNetwork(in_channels_list, out_channels)\n",
    "            self.out_channels = out_channels\n",
    "        \n",
    "        def forward(self, x):\n",
    "            outputs = self.swin_model(pixel_values=x, output_hidden_states=True)\n",
    "            feature_maps = outputs.reshaped_hidden_states\n",
    "            swin_features = collections.OrderedDict()\n",
    "            swin_features['0'] = feature_maps[0]\n",
    "            swin_features['1'] = feature_maps[1]\n",
    "            swin_features['2'] = feature_maps[2]\n",
    "            swin_features['3'] = feature_maps[3]\n",
    "            fpn_features = self.fpn(swin_features)\n",
    "            return fpn_features\n",
    "\n",
    "    backbone = SwinFPNBackbone(backbone_model, in_channels_list, out_channels)\n",
    "    anchor_generator = AnchorGenerator(\n",
    "        sizes=((32,), (64,), (128,), (256,)),\n",
    "        aspect_ratios=((0.5, 1.0, 2.0),) * 4\n",
    "    )\n",
    "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'],\n",
    "                                                    output_size=7,\n",
    "                                                    sampling_ratio=2)\n",
    "    model = FasterRCNN(backbone,\n",
    "                       num_classes=num_classes,\n",
    "                       rpn_anchor_generator=anchor_generator,\n",
    "                       box_roi_pool=roi_pooler)\n",
    "    return model\n",
    "\n",
    "class SwinUNETRMultiTask(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.encoder = Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to24-192to384-22kto1k-ft\", add_pooling_layer=False)\n",
    "        \n",
    "        # Segmentation Decoder for upsampling with Skip Connections\n",
    "        self.upsample1 = nn.ConvTranspose2d(1536, 768, kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Conv2d(1536, 768, kernel_size=3, padding=1)\n",
    "        self.upsample2 = nn.ConvTranspose2d(768, 384, kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(768, 384, kernel_size=3, padding=1)\n",
    "        self.upsample3 = nn.ConvTranspose2d(384, 192, kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(384, 192, kernel_size=3, padding=1)\n",
    "        self.upsample4 = nn.ConvTranspose2d(192, 96, kernel_size=2, stride=2)\n",
    "        self.conv4 = nn.Conv2d(192, 96, kernel_size=3, padding=1)\n",
    "        self.seg_head = nn.Conv2d(96, 1, kernel_size=1)\n",
    "\n",
    "        # Classification Head\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(1536, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.encoder(pixel_values=x, output_hidden_states=True)\n",
    "        # Extract features from the different stages of the Swin Transformer\n",
    "        features_stage1 = outputs.reshaped_hidden_states[0]\n",
    "        features_stage2 = outputs.reshaped_hidden_states[1]\n",
    "        features_stage3 = outputs.reshaped_hidden_states[2]\n",
    "        features_stage4 = outputs.reshaped_hidden_states[3]\n",
    "\n",
    "        # Segmentation pathway with skip connections\n",
    "        seg_out = self.upsample1(features_stage4)\n",
    "        seg_out = torch.cat([seg_out, features_stage3], dim=1)\n",
    "        seg_out = self.conv1(seg_out)\n",
    "        \n",
    "        seg_out = self.upsample2(seg_out)\n",
    "        seg_out = torch.cat([seg_out, features_stage2], dim=1)\n",
    "        seg_out = self.conv2(seg_out)\n",
    "        \n",
    "        seg_out = self.upsample3(seg_out)\n",
    "        seg_out = torch.cat([seg_out, features_stage1], dim=1)\n",
    "        seg_out = self.conv3(seg_out)\n",
    "        \n",
    "        seg_out = self.upsample4(seg_out)\n",
    "        seg_out = self.conv4(seg_out)\n",
    "        seg_output = self.seg_head(seg_out)\n",
    "\n",
    "        # Classification pathway\n",
    "        cls_features = self.avg_pool(features_stage4)\n",
    "        cls_output = self.cls_head(cls_features.view(cls_features.size(0), -1))\n",
    "        \n",
    "        return seg_output, cls_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a1974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_gt, y1_gt, x2_gt, y2_gt = box2\n",
    "    inter_x1 = max(x1, x1_gt)\n",
    "    inter_y1 = max(y1, y1_gt)\n",
    "    inter_x2 = min(x2, x2_gt)\n",
    "    inter_y2 = min(y2, y2_gt)\n",
    "    inter_width = max(0, inter_x2 - inter_x1)\n",
    "    inter_height = max(0, inter_y2 - inter_y1)\n",
    "    intersection = inter_width * inter_height\n",
    "    area_pred = (x2 - x1) * (y2 - y1)\n",
    "    area_gt = (x2_gt - x1_gt) * (y2_gt - y1_gt)\n",
    "    union = area_pred + area_gt - intersection\n",
    "    if union == 0:\n",
    "        return 0.0\n",
    "    return intersection / union\n",
    "\n",
    "def calculate_dice(box1, box2):\n",
    "    iou = calculate_iou(box1, box2)\n",
    "    return (2 * iou) / (1 + iou)\n",
    "\n",
    "def evaluate_faster_rcnn(model, data_loader):\n",
    "    model.eval()\n",
    "    iou_scores = []\n",
    "    dice_scores = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets, _ in data_loader:\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "            detections = model(images)\n",
    "            for i, (detection, target) in enumerate(zip(detections, targets)):\n",
    "                gt_box = target['boxes'][0].cpu().numpy()\n",
    "                if len(detection['boxes']) > 0:\n",
    "                    pred_box = detection['boxes'][0].cpu().numpy()\n",
    "                    iou = calculate_iou(pred_box, gt_box)\n",
    "                    dice = calculate_dice(pred_box, gt_box)\n",
    "                    iou_scores.append(iou)\n",
    "                    dice_scores.append(dice)\n",
    "                else:\n",
    "                    iou_scores.append(0)\n",
    "                    dice_scores.append(0)\n",
    "    if len(iou_scores) == 0:\n",
    "      return 0.0, 0.0\n",
    "    avg_iou = sum(iou_scores) / len(iou_scores)\n",
    "    avg_dice = sum(dice_scores) / len(dice_scores)\n",
    "    return avg_iou, avg_dice\n",
    "\n",
    "def evaluate_swin_unetr(model, data_loader, class_map):\n",
    "    model.eval()\n",
    "    pixel_iou_scores = []\n",
    "    pixel_dice_scores = []\n",
    "    all_true_labels = []\n",
    "    all_predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets, class_labels in data_loader:\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            for i, (image, target) in enumerate(zip(images, targets)):\n",
    "                gt_box = target['boxes'][0].cpu().int().tolist()\n",
    "                x1, y1, x2, y2 = gt_box\n",
    "                cropped_roi = F.crop(image, y1, x1, y2 - y1, x2 - x1)\n",
    "                resized_roi = F.resize(cropped_roi, (384, 384)).unsqueeze(0)\n",
    "                dummy_gt_mask = torch.zeros_like(resized_roi)\n",
    "                dummy_gt_mask[:, 50:334, 50:334] = 1 \n",
    "                seg_output, cls_logits = model(resized_roi)\n",
    "                predicted_mask = (torch.sigmoid(seg_output) > 0.5).int()\n",
    "                intersection = (predicted_mask * dummy_gt_mask).sum()\n",
    "                union = (predicted_mask + dummy_gt_mask).sum() - intersection\n",
    "                pixel_iou = intersection / union if union > 0 else 0.0\n",
    "                pixel_dice = (2. * intersection) / (predicted_mask.sum() + dummy_gt_mask.sum())\n",
    "                pixel_iou_scores.append(pixel_iou.item())\n",
    "                pixel_dice_scores.append(pixel_dice.item())\n",
    "                predicted_class = torch.argmax(cls_logits, dim=1).item()\n",
    "                true_class = class_labels[i].item()\n",
    "                all_true_labels.append(true_class)\n",
    "                all_predicted_labels.append(predicted_class)\n",
    "    accuracy = accuracy_score(all_true_labels, all_predicted_labels)\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(all_true_labels, all_predicted_labels, average='macro', zero_division=0)\n",
    "    return accuracy, precision, recall, f1_score, sum(pixel_iou_scores)/len(pixel_iou_scores), sum(pixel_dice_scores)/len(pixel_dice_scores)\n",
    "\n",
    "def run_experiment(lr, num_epochs, train_loader, test_loader, num_detection_classes, num_classification_classes, class_map):\n",
    "    faster_rcnn_model = get_faster_rcnn_model(num_classes=num_detection_classes).to(DEVICE)\n",
    "    faster_rcnn_optimizer = torch.optim.AdamW(faster_rcnn_model.parameters(), lr=lr)\n",
    "    # train_model(faster_rcnn_model, train_loader, faster_rcnn_optimizer, num_epochs)\n",
    "    iou, dice = evaluate_faster_rcnn(faster_rcnn_model, test_loader)\n",
    "    swin_unetr_model = SwinUNETRMultiTask(num_classes=num_classification_classes).to(DEVICE)\n",
    "    accuracy, precision, recall, f1_score, pixel_iou, pixel_dice = evaluate_swin_unetr(swin_unetr_model, test_loader, class_map)\n",
    "    return {\n",
    "        'learning_rate': lr,\n",
    "        'Faster R-CNN IoU': iou,\n",
    "        'Faster R-CNN Dice': dice,\n",
    "        'Swin UNETR Accuracy': accuracy,\n",
    "        'Swin UNETR Precision': precision,\n",
    "        'Swin UNETR Recall': recall,\n",
    "        'Swin UNETR F1-Score': f1_score,\n",
    "        'Swin UNETR Pixel IoU': pixel_iou,\n",
    "        'Swin UNETR Pixel Dice': pixel_dice,\n",
    "    }\n",
    "\n",
    "def plot_results(results_df):\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    results_df.plot(x='learning_rate', y=['Faster R-CNN IoU', 'Faster R-CNN Dice'], kind='bar', ax=axes[0], rot=0)\n",
    "    axes[0].set_title('Stage 1: Faster R-CNN Performance')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].set_xlabel('Learning Rate')\n",
    "    axes[0].legend(title='Metric')\n",
    "    results_df.plot(x='learning_rate', y=['Swin UNETR Accuracy', 'Swin UNETR F1-Score'], kind='bar', ax=axes[1], rot=0)\n",
    "    axes[1].set_title('Stage 2: Swin UNETR Performance')\n",
    "    axes[1].set_ylabel('Score')\n",
    "    axes[1].set_xlabel('Learning Rate')\n",
    "    axes[1].legend(title='Metric')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_model(model, data_loader, optimizer, num_epochs):\n",
    "    model.to(DEVICE)\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i, (images, targets, _) in enumerate(data_loader):\n",
    "            images = list(image.to(DEVICE) for image in images)\n",
    "            targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            total_loss += losses.item()\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] finished. Average Loss: {total_loss / len(data_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b48465",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CBISDDSMDataset(all_training)\n",
    "test_dataset = CBISDDSMDataset(all_test)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "NUM_DETECTION_CLASSES = 2 \n",
    "NUM_CLASSIFICATION_CLASSES = 2\n",
    "NUM_EPOCHS = 1\n",
    "hyperparameters = [\n",
    "    {'lr': 1e-4},\n",
    "    {'lr': 1e-5},\n",
    "    {'lr': 1e-6},\n",
    "]\n",
    "all_results = []\n",
    "\n",
    "print(\"Starting Hyperparameter Tuning...\")\n",
    "for params in hyperparameters:\n",
    "    print(f\"\\n--- Running Experiment with LR: {params['lr']} ---\")\n",
    "    results = run_experiment(\n",
    "        lr=params['lr'],\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        train_loader=train_data_loader,\n",
    "        test_loader=test_data_loader,\n",
    "        num_detection_classes=NUM_DETECTION_CLASSES,\n",
    "        num_classification_classes=NUM_CLASSIFICATION_CLASSES,\n",
    "        class_map=train_dataset.class_map\n",
    "    )\n",
    "    all_results.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8098f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "print(results_df)\n",
    "plot_results(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
